{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cocoon_Segregation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAg9BqOeOCPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a9c841-bdb1-49ea-e7a7-75049813c701"
      },
      "source": [
        " %tensorflow_version 2.x\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  import tensorflow.compat.v2 as tf\n",
        "except Exception:\n",
        "  import tensorflow as tf\n",
        "\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA42kE-6cwmW"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHAgRh29OdX6"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import shutil\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JW_R-zwOgUi"
      },
      "source": [
        "def clean_data_files():\n",
        "  if(os.path.isdir(\"dataset\")):\n",
        "    shutil.rmtree(\"dataset\", ignore_errors=False, onerror=None)\n",
        "  if(os.path.isdir(\"dev_dataset\")):\n",
        "    shutil.rmtree(\"dev_dataset\", ignore_errors=False, onerror=None)\n",
        "  if os.path.isdir(\"models\"):\n",
        "    shutil.rmtree(\"models\", ignore_errors=False, onerror=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8r0GWblOM3q"
      },
      "source": [
        "clean_data_files()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYNVijUZOm81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1dc23e67-7590-4da7-aaff-2d656b7f2f22"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqNoVSQFVptu"
      },
      "source": [
        "# Uncompress the feature images and labels csv\n",
        "def uncompress_features_labels(dir,name):\n",
        "    if(os.path.isdir(name)):\n",
        "        print('Data extracted')\n",
        "    else:\n",
        "        with ZipFile(dir) as zipf:\n",
        "            zipf.extractall(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJlUi91uVt4-"
      },
      "source": [
        "root_path = '/content/drive/My Drive/dataset/cocoon.zip'  #change dir to your project folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcDFKEetVzC6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77296dba-b6f3-4b94-b2f6-b4d5b16168b4"
      },
      "source": [
        "uncompress_features_labels(root_path,'dataset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data extracted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dwlpjxRXMWq"
      },
      "source": [
        "path = 'dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svyIAVyJXNSH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e104b13-6e33-44c4-bd9e-223388b3fb7f"
      },
      "source": [
        "data_dir = pathlib.Path(path)\n",
        "image_count = len(list(data_dir.glob('*/*/*.*')))\n",
        "image_count"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 531
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgUMJPbjYDVb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc7dd092-fca3-4bed-92dd-9402d66ea8b9"
      },
      "source": [
        "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*')])\n",
        "CLASS_NAMES"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['cocoon'], dtype='<U6')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 532
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovX7F_bcZa2Y"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "IMG_SIZE = 224\n",
        "EPOCS = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ab0GtCfauQw"
      },
      "source": [
        "def flip(x: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Flip augmentation\n",
        "\n",
        "    Args:\n",
        "        x: Image to flip\n",
        "\n",
        "    Returns:\n",
        "        Augmented image\n",
        "    \"\"\"\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_flip_up_down(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3rjL8-DaxoM"
      },
      "source": [
        "def color(x: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Color augmentation\n",
        "\n",
        "    Args:\n",
        "        x: Image\n",
        "\n",
        "    Returns:\n",
        "        Augmented image\n",
        "    \"\"\"\n",
        "    x = tf.image.random_hue(x, 0.08)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.6)\n",
        "    x = tf.image.random_brightness(x, 0.05)\n",
        "    x = tf.image.random_contrast(x, 0.7, 1.3)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I7KKwGea0r4"
      },
      "source": [
        "def rotate(x: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Rotation augmentation\n",
        "\n",
        "    Args:\n",
        "        x: Image\n",
        "\n",
        "    Returns:\n",
        "        Augmented image\n",
        "    \"\"\"\n",
        "    return tf.image.rot90(x, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1u8TzGJa3uu"
      },
      "source": [
        "def zoom(x: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Zoom augmentation\n",
        "\n",
        "    Args:\n",
        "        x: Image\n",
        "\n",
        "    Returns:\n",
        "        Augmented image\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate 20 crop settings, ranging from a 1% to 20% crop.\n",
        "    scales = list(np.arange(0.8, 1.0, 0.01))\n",
        "    boxes = np.zeros((len(scales), 4))\n",
        "\n",
        "    for i, scale in enumerate(scales):\n",
        "        x1 = y1 = 0.5 - (0.5 * scale)\n",
        "        x2 = y2 = 0.5 + (0.5 * scale)\n",
        "        boxes[i] = [x1, y1, x2, y2]\n",
        "\n",
        "    def random_crop(img):\n",
        "        # Create different crops for an image\n",
        "        crops = tf.image.crop_and_resize([img], boxes=boxes, box_indices=np.zeros(len(scales)), crop_size=(IMG_SIZE, IMG_SIZE))\n",
        "        # Return a random crop\n",
        "        return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
        "\n",
        "\n",
        "    choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
        "\n",
        "    # Only apply cropping 50% of the time\n",
        "    return tf.cond(choice < 0.5, lambda: x, lambda: random_crop(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W_ufPxva_Fa"
      },
      "source": [
        "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*/*.*'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCM_GbxnbFyU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "81b69006-7e29-4aab-f7ba-c28d3286adc2"
      },
      "source": [
        "for f in list_ds.take(2):\n",
        "  print(f.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'dataset/cocoon/bad_ones/5.png'\n",
            "b'dataset/cocoon/good_ones/g13.jpeg'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc0lfiBCbMD8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "895ca35d-6da1-40f6-e072-d46c2237a444"
      },
      "source": [
        "train_size = int(0.7 * image_count)\n",
        "val_size = int(0.2 * image_count)\n",
        "test_size = image_count - train_size - val_size\n",
        "\n",
        "print(\"Total Images     : \", image_count)\n",
        "print(\"train Images     : \", train_size)\n",
        "print(\"validation Images: \", val_size)\n",
        "print(\"test Images      : \", test_size)\n",
        "\n",
        "SUFFLE_BUFFER_SIZE = int(test_size/2)\n",
        "STEPS_PER_EPOCH = np.ceil(train_size/BATCH_SIZE)\n",
        "VALIDATION_STEPS = np.ceil(val_size/BATCH_SIZE)\n",
        "\n",
        "full_list_dataset = list_ds.shuffle(buffer_size=SUFFLE_BUFFER_SIZE)\n",
        "train_list_dataset = full_list_dataset.take(train_size)\n",
        "test_list_dataset = full_list_dataset.skip(train_size)\n",
        "val_list_dataset = test_list_dataset.take(val_size)\n",
        "test_list_dataset = test_list_dataset.skip(val_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Images     :  26\n",
            "train Images     :  18\n",
            "validation Images:  5\n",
            "test Images      :  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHEr_hb3bQDM"
      },
      "source": [
        "def get_label(file_path):\n",
        "  # convert the path to a list of path components\n",
        "  parts = tf.strings.split(file_path, os.path.sep)\n",
        "  # # The second to last is the class-directory\n",
        "  return parts[-2] == CLASS_NAMES "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdH6iBAjbe1K"
      },
      "source": [
        "def decode_img(img):\n",
        "  # convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  # resize the image to the desired size.\n",
        "  return tf.image.resize(img, [IMG_SIZE, IMG_SIZE])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh21tDnAbhSZ"
      },
      "source": [
        "def process_path(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4_ZGZKbj-E"
      },
      "source": [
        "def process_path_flip(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  img = flip(img)\n",
        "  return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEjgnWa2bmFi"
      },
      "source": [
        "def process_path_color(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  img = color(img)\n",
        "  return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN5ODK4SbmEI"
      },
      "source": [
        "def process_path_rotate(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  img = rotate(img)\n",
        "  return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB8v2r7Abq4Y"
      },
      "source": [
        "def process_path_zoom(file_path):\n",
        "  label = get_label(file_path)\n",
        "  # load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  img = zoom(img)\n",
        "  return img, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms2CL2dRbyaW"
      },
      "source": [
        "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
        "labeled_normal_ds = train_list_dataset.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "labeled_flipped_ds = train_list_dataset.map(process_path_flip, num_parallel_calls=AUTOTUNE)\n",
        "labeled_color_ds = train_list_dataset.map(process_path_color, num_parallel_calls=AUTOTUNE)\n",
        "labeled_rotate_ds = train_list_dataset.map(process_path_rotate, num_parallel_calls=AUTOTUNE)\n",
        "labeled_zoomed_ds = train_list_dataset.map(process_path_zoom, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "train_dataset = labeled_normal_ds\n",
        "train_dataset = train_dataset.concatenate(labeled_flipped_ds)\n",
        "train_dataset = train_dataset.concatenate(labeled_flipped_ds)\n",
        "train_dataset = train_dataset.concatenate(labeled_rotate_ds)\n",
        "train_dataset = train_dataset.concatenate(labeled_zoomed_ds)\n",
        "\n",
        "\n",
        "labeled_normal_ds = val_list_dataset.map(process_path, num_parallel_calls=AUTOTUNE)\n",
        "labeled_flipped_ds = val_list_dataset.map(process_path_flip, num_parallel_calls=AUTOTUNE)\n",
        "labeled_color_ds = val_list_dataset.map(process_path_color, num_parallel_calls=AUTOTUNE)\n",
        "labeled_rotate_ds = val_list_dataset.map(process_path_rotate, num_parallel_calls=AUTOTUNE)\n",
        "labeled_zoomed_ds = val_list_dataset.map(process_path_zoom, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "val_dataset = labeled_normal_ds\n",
        "val_dataset = val_dataset.concatenate(labeled_flipped_ds)\n",
        "val_dataset = val_dataset.concatenate(labeled_flipped_ds)\n",
        "val_dataset = val_dataset.concatenate(labeled_rotate_ds)\n",
        "val_dataset = val_dataset.concatenate(labeled_zoomed_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pSVBHejdIMn"
      },
      "source": [
        "def prepare_for_training(ds, cache=False, shuffle_buffer_size=SUFFLE_BUFFER_SIZE):\n",
        "  # This is a small dataset, only load it once, and keep it in memory.\n",
        "  # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
        "  # fit in memory.\n",
        "  if cache:\n",
        "    if isinstance(cache, str):\n",
        "      ds = ds.cache(cache)\n",
        "    else:\n",
        "      ds = ds.cache()\n",
        "\n",
        "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "  # Repeat forever\n",
        "  ds = ds.repeat()\n",
        "\n",
        "  ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
        "  # is training.\n",
        "  ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cww67ZA1gnLd"
      },
      "source": [
        "train_ds = prepare_for_training(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61np568stk1b"
      },
      "source": [
        "train_image_batch, train_label_batch = next(iter(train_ds))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmV7nvjbrMh_"
      },
      "source": [
        "val_ds = prepare_for_training(val_dataset)\n",
        "\n",
        "val_image_batch, val_label_batch = next(iter(val_ds))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPOBOuOWrSCc"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, MaxPool2D, Dropout, Flatten, Activation, MaxPooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bi2TEvsqWbx"
      },
      "source": [
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "mobilenet_v2 = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", input_shape=IMG_SHAPE, trainable=False)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  mobilenet_v2,\n",
        "#  Dense(16, activation='relu'),\n",
        "  Dense(4, activation='relu'),\n",
        "  Dense(2, activation='softmax')\n",
        "])\n",
        "model.build([None, 224, 224, 3])  # Batch input shape."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ6_rqEDqnZP"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv63g0zxqhrf"
      },
      "source": [
        "if not os.path.exists('models'):\n",
        "  os.mkdir('models')\n",
        "\n",
        "if not os.path.exists('models/checkpoints'):\n",
        "  os.mkdir('models/checkpoints/')\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import  ModelCheckpoint\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"models/checkpoints/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB2YlZ2Vqj8B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "f87ba5ea-2a5a-4b26-a05b-055f1d46d9cb"
      },
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    epochs=EPOCS,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=VALIDATION_STEPS,\n",
        "    #callbacks=callbacks_list\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 5.0 steps, validate for 2.0 steps\n",
            "Epoch 1/2\n",
            "5/5 [==============================] - 6s 1s/step - loss: 0.0000e+00 - accuracy: 0.7500 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 2/2\n",
            "5/5 [==============================] - 1s 270ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvpp6ivda_ti",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "74612c65-5421-42dd-fc29-a779c8e8bd45"
      },
      "source": [
        "#Prediction\n",
        "#model.save('cocoon.tflite')\n",
        "model.save('cocoon.model')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cocoon.model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: cocoon.model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8bSSan2eobC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d61af0da-b726-4ed7-aba9-63ccaf892440"
      },
      "source": [
        "Cat = ['Defect','No Defect']\n",
        "def prepare(filepath):\n",
        "  IMG_SIZE = 224\n",
        "  img_array = cv2.imread(filepath, cv2.IMREAD_COLOR)\n",
        "  new_array = cv2.resize(img_array, (IMG_SIZE,IMG_SIZE))\n",
        "  return new_array.reshape(-1, IMG_SIZE,IMG_SIZE, 3)\n",
        "\n",
        "model = tf.keras.models.load_model('cocoon.tflite')\n",
        "\n",
        "prediction = model.predict([prepare('test1.jpg')])\n",
        "a=prediction[0][0]\n",
        "b=prediction[0][1]\n",
        "#pred.append(prediction)\n",
        "if round(a,3)>round(b,3):\n",
        "  print(round(a,3))\n",
        "  print(Cat[math.floor(a)])\n",
        "else:\n",
        "  print(round(b,3))\n",
        "  print(Cat[math.ceil(b)])\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.78\n",
            "No Defect\n",
            "[[0.21975358 0.78024644]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9-CsQHAhWOh"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKeSH47ehWNk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7226ccf9-6348-46a1-b09f-fdb20cec2106"
      },
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "  Cat = ['Defect','No Defect']\n",
        "  def prepare(filepath):\n",
        "    IMG_SIZE = 224\n",
        "    img_array = cv2.imread(filepath, cv2.IMREAD_COLOR)\n",
        "    new_array = cv2.resize(img_array, (IMG_SIZE,IMG_SIZE))\n",
        "    return new_array.reshape(-1, IMG_SIZE,IMG_SIZE, 3)\n",
        "\n",
        "  #model = tf.keras.models.load_model('dataset/final3.model')\n",
        "\n",
        "  prediction = model.predict([prepare(filename)])\n",
        " # print(Cat[int(prediction[0][0])])\n",
        " # print(prediction)\n",
        "  a=prediction[0][0]\n",
        "  b=prediction[0][1]\n",
        "#pred.append(prediction)\n",
        "  if round(a,3)>round(b,3):\n",
        "    print(round(a,3))\n",
        "    print(Cat[0])\n",
        "  else:\n",
        "    print(round(b,3))\n",
        "    print(Cat[1])\n",
        "  print(prediction)\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name 'take_photo' is not defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tiq-zKBbwIr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}